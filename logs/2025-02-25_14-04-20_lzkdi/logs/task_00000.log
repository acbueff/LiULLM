2025-02-25 14:04:20.667 | INFO     | datatrove.utils.logging:add_task_logger:58 - Launching pipeline for rank=0
2025-02-25 14:04:20.667 | INFO     | datatrove.utils.logging:log_pipeline:90 - 
--- ğŸ› ï¸ PIPELINE ğŸ› 
ğŸ“– - READER: ğŸ“’ Parquet
ğŸ”» - FILTER: ğŸ‘¤ Lambda
ğŸ’½ - WRITER: ğŸ“’ Parquet
2025-02-25 14:04:22.182 | ERROR    | datatrove.executor.base:_run_for_rank:108 - 'type'
Traceback (most recent call last):

  File "/home/andbu/Documents/trustllm/TechStack/LiULLM/JUWELS/WP2/data/get_swed_2.py", line 273, in <module>
    download_in_stages(
    â”” <function download_in_stages at 0x7fae97377060>

  File "/home/andbu/Documents/trustllm/TechStack/LiULLM/JUWELS/WP2/data/get_swed_2.py", line 254, in download_in_stages
    pipeline_executor.run()
    â”‚                 â”” <function LocalPipelineExecutor.run at 0x7fae97367b00>
    â”” <datatrove.executor.local.LocalPipelineExecutor object at 0x7fae9751a750>

  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/executor/local.py", line 127, in run
    stats.append(self._launch_run_for_rank(rank, ranks_q))
    â”‚     â”‚      â”‚    â”‚                    â”‚     â”” <AutoProxy[Queue] object, typeid 'Queue' at 0x7fae5a1f10d0>
    â”‚     â”‚      â”‚    â”‚                    â”” 0
    â”‚     â”‚      â”‚    â”” <function LocalPipelineExecutor._launch_run_for_rank at 0x7fae98cb3ec0>
    â”‚     â”‚      â”” <datatrove.executor.local.LocalPipelineExecutor object at 0x7fae9751a750>
    â”‚     â”” <method 'append' of 'list' objects>
    â”” []
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/executor/local.py", line 76, in _launch_run_for_rank
    return self._run_for_rank(rank, local_rank)
           â”‚    â”‚             â”‚     â”” 0
           â”‚    â”‚             â”” 0
           â”‚    â”” <function PipelineExecutor._run_for_rank at 0x7fae97367600>
           â”” <datatrove.executor.local.LocalPipelineExecutor object at 0x7fae9751a750>
> File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/executor/base.py", line 96, in _run_for_rank
    deque(pipelined_data, maxlen=0)
    â”‚     â”” <generator object DiskWriter.run at 0x7fae59e1b120>
    â”” <class 'collections.deque'>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/writers/disk_base.py", line 178, in run
    for document in data:
                    â”” <generator object BaseFilter.run at 0x7fae5a1efec0>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/filters/base_filter.py", line 64, in run
    for batch in batched(data, self.batch_size):
                 â”‚       â”‚     â”‚    â”” 1
                 â”‚       â”‚     â”” ğŸ”» - FILTER: ğŸ‘¤ Lambda
                 â”‚       â”” <generator object BaseDiskReader.run at 0x7fae59e1b010>
                 â”” <function batched at 0x7fae5a3dd800>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/utils/batching.py", line 20, in batched
    while batch := list(itertools.islice(it, n)):
                        â”‚         â”‚      â”‚   â”” 1
                        â”‚         â”‚      â”” <generator object BaseDiskReader.run at 0x7fae59e1b010>
                        â”‚         â”” <class 'itertools.islice'>
                        â”” <module 'itertools' (built-in)>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/readers/base.py", line 223, in run
    self.data_folder.get_shard(rank, world_size, recursive=self.recursive, glob_pattern=self.glob_pattern)
    â”‚    â”‚           â”‚         â”‚     â”‚                     â”‚    â”‚                       â”‚    â”” None
    â”‚    â”‚           â”‚         â”‚     â”‚                     â”‚    â”‚                       â”” ğŸ“– - READER: ğŸ“’ Parquet
    â”‚    â”‚           â”‚         â”‚     â”‚                     â”‚    â”” True
    â”‚    â”‚           â”‚         â”‚     â”‚                     â”” ğŸ“– - READER: ğŸ“’ Parquet
    â”‚    â”‚           â”‚         â”‚     â”” 1
    â”‚    â”‚           â”‚         â”” 0
    â”‚    â”‚           â”” <function DataFolder.get_shard at 0x7fae97525da0>
    â”‚    â”” DataFolder(path='datasets/HuggingFaceFW/fineweb-2/data/swe_Latn/train/000_00000.parquet', fs=<huggingface_hub.hf_file_system....
    â”” ğŸ“– - READER: ğŸ“’ Parquet
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/io.py", line 178, in get_shard
    all_files = self.list_files(**kwargs)
                â”‚    â”‚            â”” {'recursive': True, 'glob_pattern': None}
                â”‚    â”” <function DataFolder.list_files at 0x7fae97525d00>
                â”” DataFolder(path='datasets/HuggingFaceFW/fineweb-2/data/swe_Latn/train/000_00000.parquet', fs=<huggingface_hub.hf_file_system....
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/io.py", line 149, in list_files
    [
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/io.py", line 161, in <listcomp>
    if include_directories or info["type"] != "directory"
       â”‚                      â”” {}
       â”” False

KeyError: 'type'
