2025-02-25 14:16:58.895 | INFO     | datatrove.utils.logging:add_task_logger:58 - Launching pipeline for rank=1
2025-02-25 14:16:58.895 | INFO     | datatrove.utils.logging:log_pipeline:90 - 
--- ðŸ› ï¸ PIPELINE ðŸ› 
ðŸ“– - READER: ðŸ“’ Parquet
ðŸ”» - FILTER: ðŸ‘¤ Lambda
ðŸ’½ - WRITER: ðŸ“’ Parquet
2025-02-25 14:17:00.234 | INFO     | datatrove.pipeline.readers.base:read_files_shard:191 - Reading input file 000_00001.parquet, 1/7
2025-02-25 14:17:02.461 | ERROR    | datatrove.executor.base:_run_for_rank:108 - '>=' not supported between instances of 'int' and 'NoneType'
Traceback (most recent call last):

  File "<string>", line 1, in <module>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/forkserver.py", line 273, in main
    code = _serve_one(child_r, fds,
           â”‚          â”‚        â”” [10, 11, 12, 13, 14, 15]
           â”‚          â”” 8
           â”” <function _serve_one at 0x7f92f09402c0>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/forkserver.py", line 312, in _serve_one
    code = spawn._main(child_r, parent_sentinel)
           â”‚     â”‚     â”‚        â”” 4
           â”‚     â”‚     â”” 8
           â”‚     â”” <function _main at 0x7f92f092f420>
           â”” <module 'multiprocess.spawn' from '/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/spawn.py'>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/spawn.py", line 135, in _main
    return self._bootstrap(parent_sentinel)
           â”‚    â”‚          â”” 4
           â”‚    â”” <function BaseProcess._bootstrap at 0x7f92f0ea8b80>
           â”” <ForkServerProcess name='ForkServerPoolWorker-2' parent=30960 started daemon>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
    â”‚    â”” <function BaseProcess.run at 0x7f92f0ea80e0>
    â”” <ForkServerProcess name='ForkServerPoolWorker-2' parent=30960 started daemon>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    â”‚    â”‚        â”‚    â”‚        â”‚    â”” {}
    â”‚    â”‚        â”‚    â”‚        â”” <ForkServerProcess name='ForkServerPoolWorker-2' parent=30960 started daemon>
    â”‚    â”‚        â”‚    â”” (<multiprocess.queues.SimpleQueue object at 0x7f92ef30cbd0>, <multiprocess.queues.SimpleQueue object at 0x7f92d213ca10>, None...
    â”‚    â”‚        â”” <ForkServerProcess name='ForkServerPoolWorker-2' parent=30960 started daemon>
    â”‚    â”” <function worker at 0x7f92b1d1b740>
    â”” <ForkServerProcess name='ForkServerPoolWorker-2' parent=30960 started daemon>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    â”‚     â”‚       â”” {}
                    â”‚     â”” (1,)
                    â”” functools.partial(<bound method LocalPipelineExecutor._launch_run_for_rank of <datatrove.executor.local.LocalPipelineExecutor...
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/executor/local.py", line 76, in _launch_run_for_rank
    return self._run_for_rank(rank, local_rank)
           â”‚    â”‚             â”‚     â”” 1
           â”‚    â”‚             â”” 1
           â”‚    â”” <function PipelineExecutor._run_for_rank at 0x7f92ef25f560>
           â”” <datatrove.executor.local.LocalPipelineExecutor object at 0x7f92f104d650>
> File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/executor/base.py", line 96, in _run_for_rank
    deque(pipelined_data, maxlen=0)
    â”‚     â”” <generator object DiskWriter.run at 0x7f92b1cf3bc0>
    â”” <class 'collections.deque'>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/writers/disk_base.py", line 178, in run
    for document in data:
                    â”” <generator object BaseFilter.run at 0x7f92b1d487c0>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/filters/base_filter.py", line 64, in run
    for batch in batched(data, self.batch_size):
                 â”‚       â”‚     â”‚    â”” 1
                 â”‚       â”‚     â”” ðŸ”» - FILTER: ðŸ‘¤ Lambda
                 â”‚       â”” <generator object BaseDiskReader.run at 0x7f92f104adf0>
                 â”” <function batched at 0x7f92d211d6c0>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/utils/batching.py", line 20, in batched
    while batch := list(itertools.islice(it, n)):
                        â”‚         â”‚      â”‚   â”” 1
                        â”‚         â”‚      â”” <generator object BaseDiskReader.run at 0x7f92f104adf0>
                        â”‚         â”” <class 'itertools.islice'>
                        â”” <module 'itertools' (built-in)>
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/readers/base.py", line 235, in run
    for doc in self.read_files_shard(files_shard):
               â”‚    â”‚                â”” ['000_00001.parquet', '001_00000.parquet', '001_00002.parquet', '002_00001.parquet', '003_00000.parquet', '003_00002.parquet'...
               â”‚    â”” <function BaseDiskReader.read_files_shard at 0x7f92ef270cc0>
               â”” ðŸ“– - READER: ðŸ“’ Parquet
  File "/home/andbu/miniconda3/envs/llms/lib/python3.11/site-packages/datatrove/pipeline/readers/base.py", line 198, in read_files_shard
    if self.limit != -1 and li >= self.limit:
       â”‚    â”‚               â”‚     â”‚    â”” None
       â”‚    â”‚               â”‚     â”” ðŸ“– - READER: ðŸ“’ Parquet
       â”‚    â”‚               â”” 0
       â”‚    â”” None
       â”” ðŸ“– - READER: ðŸ“’ Parquet

TypeError: '>=' not supported between instances of 'int' and 'NoneType'
